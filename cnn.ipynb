{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nel_1\\AppData\\Local\\Temp\\ipykernel_10892\\3433365118.py:2: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  import imp\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "import imp\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from sklearn import svm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    23481\n",
      "0    21417\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "true_df = pd.read_csv('./True.csv')\n",
    "fake_df = pd.read_csv('./Fake.csv')\n",
    "true_df['label'] = 0\n",
    "fake_df['label'] = 1\n",
    "true_df = true_df[['text','label']]\n",
    "fake_df = fake_df[['text','label']]\n",
    "dataset = pd.concat([true_df , fake_df])\n",
    "print(dataset['label'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading our data in to dataframes, it's time to clean it.</br>\n",
    "to cleanse this data a function is defined to convert the upper case to lower case, substitute non English words with space and tokenize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.sample(frac = 1)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = set(stopwords.words('english'))\n",
    "def clean_data(text):\n",
    "    text = text.lower() \n",
    "    text = re.sub('[^a-zA-Z]' , ' ' , text)\n",
    "    token = text.split() \n",
    "    news = [lemmatizer.lemmatize(word) for word in token if not word in stopwords]  \n",
    "    clean_news = ' '.join(news) \n",
    "    \n",
    "    return clean_news "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>\n",
    "</br>\n",
    "\n",
    "to save runtime, we only read 20000 rows of our dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4995     reuters u president donald trump nominee head ...\n",
      "2792     donald trump got as handed gold star father si...\n",
      "6790     itching taste life america president donald tr...\n",
      "6692     woman make percent country grossly underrepres...\n",
      "10934    knew tiffany co decided show hand come politic...\n",
      "Name: text, dtype: object\n",
      "4995     0\n",
      "2792     1\n",
      "6790     1\n",
      "6692     1\n",
      "10934    1\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataset['text'] = dataset['text'].apply(lambda x : clean_data(x))\n",
    "vectorizer = TfidfVectorizer(max_features = 50000 , lowercase=False , ngram_range=(1,2))\n",
    "X = dataset.iloc[:10000,0]\n",
    "y = dataset.iloc[:10000,1]\n",
    "print(X.head())\n",
    "print(y.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next step is to split our data into train and test and use vectorizer on them to map words or phrases from vocabulary to a corresponding vector of real numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X , test_X , train_y , test_y = train_test_split(X , y , test_size = 0.2 ,random_state = 0)\n",
    "vec_train = vectorizer.fit_transform(train_X)\n",
    "vec_train = vec_train.toarray()\n",
    "vec_test = vectorizer.transform(test_X).toarray()\n",
    "train_data = pd.DataFrame(vec_train , columns=vectorizer.get_feature_names_out())\n",
    "test_data = pd.DataFrame(vec_test , columns= vectorizer.get_feature_names_out())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implementing the model and the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "250/250 [==============================] - 12s 45ms/step - loss: 0.1724 - accuracy: 0.9589\n",
      "Epoch 2/5\n",
      "250/250 [==============================] - 11s 44ms/step - loss: 0.0049 - accuracy: 0.9992\n",
      "Epoch 3/5\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 0.0014 - accuracy: 0.9998\n",
      "Epoch 4/5\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 0.0016 - accuracy: 0.9999\n",
      "Epoch 5/5\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 6.5390e-04 - accuracy: 0.9999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cf442d7d60>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units = 100 , activation = 'relu' , input_dim = vec_train.shape[1]))\n",
    "model.add(Dense(units = 50 , activation = 'relu'))\n",
    "model.add(Dense(units = 25 , activation = 'relu'))\n",
    "model.add(Dense(units = 10 , activation = 'relu'))\n",
    "model.add(Dense(units = 1 , activation = 'sigmoid'))\n",
    "model.compile(optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n",
    "model.fit(vec_train,train_y , epochs = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afb617c5004eb51001b1261fe8fb1e311bfd962946fd4f9d54afee99a9ffc8a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
